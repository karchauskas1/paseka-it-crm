import { NextResponse } from 'next/server'
import { getCurrentUser } from '@/lib/auth'
import { db } from '@/lib/db'

// Available AI models via OpenRouter (sorted by cost-efficiency)
const AI_MODELS = {
  'google/gemini-2.0-flash-exp:free': {
    name: 'Gemini 2.0 Flash ‚ö° FREE',
    provider: 'Google',
    description: '–ë–ï–°–ü–õ–ê–¢–ù–ê–Ø –º–æ–¥–µ–ª—å! –ë—ã—Å—Ç—Ä–∞—è –∏ —É–º–Ω–∞—è',
  },
  'anthropic/claude-3-haiku': {
    name: 'Claude 3 Haiku üí∞',
    provider: 'Anthropic',
    description: '–î–µ—à—ë–≤–∞—è ($0.25/1M), –±—ã—Å—Ç—Ä–∞—è –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è',
  },
  'anthropic/claude-3.5-sonnet': {
    name: 'Claude 3.5 Sonnet',
    provider: 'Anthropic',
    description: '–õ—É—á—à–∏–π –±–∞–ª–∞–Ω—Å —Ü–µ–Ω—ã/–∫–∞—á–µ—Å—Ç–≤–∞ ($3/1M)',
  },
  'openai/gpt-3.5-turbo': {
    name: 'GPT-3.5 Turbo',
    provider: 'OpenAI',
    description: '–≠–∫–æ–Ω–æ–º–∏—á–Ω–∞—è –º–æ–¥–µ–ª—å OpenAI',
  },
  'google/gemini-pro': {
    name: 'Gemini Pro',
    provider: 'Google',
    description: '–ú–æ—â–Ω–∞—è –º–æ–¥–µ–ª—å Google',
  },
  'meta-llama/llama-3-70b-instruct': {
    name: 'Llama 3 70B',
    provider: 'Meta',
    description: '–û—Ç–∫—Ä—ã—Ç–∞—è –º–æ–¥–µ–ª—å Meta',
  },
  'openai/gpt-4-turbo': {
    name: 'GPT-4 Turbo üíé',
    provider: 'OpenAI',
    description: '–î–û–†–û–ì–ê–Ø! –°–∞–º–∞—è –º–æ—â–Ω–∞—è ($10/1M)',
  },
}

const OPENROUTER_API_KEY = process.env.OPENROUTER_API_KEY
const OPENROUTER_BASE_URL = 'https://openrouter.ai/api/v1'

export async function POST(request: Request) {
  try {
    const user = await getCurrentUser()
    if (!user) {
      return NextResponse.json({ error: 'Unauthorized' }, { status: 401 })
    }

    if (!OPENROUTER_API_KEY) {
      return NextResponse.json(
        { error: 'OPENROUTER_API_KEY –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω' },
        { status: 500 }
      )
    }

    const { messages, model } = await request.json()

    if (!messages || !Array.isArray(messages)) {
      return NextResponse.json(
        { error: 'messages is required and must be an array' },
        { status: 400 }
      )
    }

    // Default to free Gemini model for cost savings
    const selectedModel = model || 'google/gemini-2.0-flash-exp:free'

    if (!AI_MODELS[selectedModel as keyof typeof AI_MODELS]) {
      return NextResponse.json(
        { error: 'Invalid model selected' },
        { status: 400 }
      )
    }

    const response = await fetch(`${OPENROUTER_BASE_URL}/chat/completions`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${OPENROUTER_API_KEY}`,
        'Content-Type': 'application/json',
        'HTTP-Referer': process.env.NEXT_PUBLIC_APP_URL || 'http://localhost:3000',
      },
      body: JSON.stringify({
        model: selectedModel,
        messages,
        max_tokens: 2000,
        stream: false,
      }),
    })

    if (!response.ok) {
      const errorText = await response.text()
      let errorMessage = `OpenRouter API error: ${response.status}`

      if (response.status === 401) {
        errorMessage = '–ù–µ–≤–µ—Ä–Ω—ã–π API –∫–ª—é—á OpenRouter'
      } else if (response.status === 402) {
        errorMessage = '–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å—Ä–µ–¥—Å—Ç–≤ –Ω–∞ —Å—á—ë—Ç–µ OpenRouter'
      } else if (response.status === 429) {
        errorMessage = '–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤'
      } else {
        try {
          const errorJson = JSON.parse(errorText)
          errorMessage = errorJson.error?.message || errorMessage
        } catch {
          errorMessage = `${errorMessage} - ${errorText}`
        }
      }

      return NextResponse.json({ error: errorMessage }, { status: response.status })
    }

    const data = await response.json()
    const content = data.choices[0]?.message?.content || ''

    return NextResponse.json({
      content,
      model: selectedModel,
      modelInfo: AI_MODELS[selectedModel as keyof typeof AI_MODELS],
      usage: data.usage,
    })
  } catch (error: any) {
    console.error('AI Chat Error:', error)
    return NextResponse.json(
      { error: error.message || '–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞' },
      { status: 500 }
    )
  }
}

// GET endpoint to return available models
export async function GET() {
  try {
    const user = await getCurrentUser()
    if (!user) {
      return NextResponse.json({ error: 'Unauthorized' }, { status: 401 })
    }

    return NextResponse.json({ models: AI_MODELS })
  } catch (error) {
    console.error('Get models error:', error)
    return NextResponse.json(
      { error: 'Internal server error' },
      { status: 500 }
    )
  }
}
